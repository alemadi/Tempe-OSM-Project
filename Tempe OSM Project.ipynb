{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "I chose the city of Tempe, AZ as the subject of this project, for sevreal reasons, I have had lived there for 5 years, therfore I have a good knowlege of the area. Intially, I wanted to do on my hometown, Doha, but after I downloaded the raw data it looked that the data has not been updated in a while, also I have noticed that I will have a hard time with the Arabic letters in the data, so I resorted to do the project on Tempe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the nature of the data\n",
    "**Note: the template of the code was provided by udacity quizzes**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': 318952, 'member': 27051, 'nd': 407790, 'tag': 279359, 'bounds': 1, 'note': 1, 'meta': 1, 'relation': 837, 'way': 51849, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import csv, codecs, pprint, re, cerberus, operator\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "file_path = 'Phoenix'\n",
    "\n",
    "def get_element(file_path):\n",
    "    \n",
    "    context = ET.iterparse(file_path, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end': \n",
    "            yield elem\n",
    "            root.clear\n",
    "    \n",
    "\n",
    "# Count tags\n",
    "def count_tags(file_in):\n",
    "    \"\"\"\n",
    "    Fill out the count_tags function. It should return a dictionary with the \n",
    "    tag name as the key and number of times this tag can be encountered in \n",
    "    the map as value.\n",
    "    \"\"\"\n",
    "    \n",
    "    tags = {}\n",
    "    \n",
    "    for event, element in ET.iterparse(file_in):\n",
    "        if element.tag in tags:\n",
    "            tags[element.tag] += 1\n",
    "            \n",
    "        else:\n",
    "            tags[element.tag] = 1\n",
    "    \n",
    "\n",
    "    return tags\n",
    "        \n",
    "print count_tags(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will use Regex to find how the data is formatted.\n",
    "3 patters are provided: <Enter> \n",
    "\n",
    "1- \"lower\", for tags that contain only lowercase letters and are valid. <Enter> \n",
    "\n",
    "2- \"lower_colon\", for otherwise valid tags with a colon in their names.\n",
    "\n",
    "3- \"problemchars\", for tags with problematic characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 187453, 'lower_colon': 88074, 'other': 3831, 'problemchars': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "       \n",
    "        # use REGEX for lower\n",
    "        if lower.search(element.attrib['k']):\n",
    "            keys['lower'] += 1\n",
    "            \n",
    "        # Use Regex for lower_colon\n",
    "        elif lower_colon.search(element.attrib['k']) :\n",
    "            keys['lower_colon'] += 1\n",
    "            \n",
    "        # Use Regex for Problem Characters\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys['problemchars'] += 1\n",
    "            \n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "process_map(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will discover the most contributed users (by username) for the Phoenix metropolitan street map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pluton_od': 135, 'pinkythepig': 2, 'Sundance': 3002, 'Brian@Brea': 55, 'ashleyannmathew': 10, 'srsjojo': 65, 'OSMF Redaction Account': 11, 'robinsonsenterprises': 1, 'woowoo256': 14, 'RichRico': 1228, 'xybot': 5, 'Sarr_Cat': 89, 'ramyaragupathy': 216, 'cgu66': 3, 'Joe2oh': 11, 'menuderiaguanajuato': 1, 'Manu1400': 1, 'Meercatnip': 1, 'The Mapster': 66, 'yurasi': 843, 'Stormy Thomas': 1, 'Stephen214': 88, 'Ollie': 95, 'Pnrrth': 14, 'JayHen': 2, 'maxerickson': 263, 'skquinn': 3, '25or6to4': 45, 'j03lar50n': 1, 'sivan00': 5, 'pilotrobert': 6, 'azstumbler': 2, 'wd1933c': 4, 'osmjwh': 25, 'Your Village Maps': 2031, 'ArizonaMapper': 431, 'me0': 2, 'Marco | Wimmer PLLC': 1, 'saikabhi': 72, 'odna': 3, 'Kadou88': 1, 'bondah': 1, 'BuganiniQ': 1, 'fidcastro': 2, 'Minh Nguyen': 2, 'LeTopographeFou': 1001, 'Margter': 1, 'FvGordon': 16, 'Travi5': 23, u'\\u042e\\u043a\\u0430\\u0442\\u0430\\u043d': 30582, 'Christy Allen': 10, 'oldtopos': 20, 'thesuntheory': 48, 'woodpeck_repair': 6, 'Spanholz': 536, 'kghazi': 691, 'jmtaysom': 2, 'punkzebra': 24, 'sunil0780': 1, 'n7xsd': 3, 'ediyes': 475, 'namannik': 424, 'nmixter': 602, 'matthewmspace': 1, 'stucki1': 1, 'ThomasBulwark': 6, 'Urfin78': 2, 'Cradleguard': 10, 'n76': 14, 'uboot': 4, 'Bryce C Nesbitt': 19, 'Ryan Lash': 3, 'apburnes': 319, 'MrRagerPager': 160, 'coffeeforclosers1': 2, 'us-az-mesa-UGBL': 161, 'qaznable': 1, 'nativenicole': 1, 'nikhilprabhakar': 448, 'bn2010': 1, 'matthieun': 2007, 'Courtney Nicole': 1, 'osmmaker': 1, 'ilanboy': 1, 'MikeN': 42, 'rhrubin': 1, 'Cheng Wang': 28, 'ChrisZontine': 3, 'RGimbel': 46, 'Gravatite': 4, 'jammypixpoodle': 4, 'aliandpotato3': 25, 'ruthmaben': 372, 'AM909': 1, 'jacobbraeutigam': 1, 'DoubleA': 6, 'Polyglot': 15, 'truckaccessplusaz': 1, 'somehow_different': 318, 'vnixon': 811, '4b696d': 17, 'MGdalyn07': 8, 'F_H_Howler': 5, 'lukas64': 23746, 'Joe Manna': 5, 'Bootprint': 1, 'Steven Vance': 3, 'reddy4it77': 91, 'DesertNavigator': 1529, u'Walter Schl\\xf6gl': 1, 'nolessofjess': 14, 'dbaron': 11, 'Glassman': 2, 'wdbspephd': 109, 'andygol': 3, 'Tom Morris': 1, 'rayn': 302, 'azcreditlawyer': 1, 'davidearl': 59, 'dchiles': 4, 'Jon Hanson': 164, 'wheelmap_visitor': 8, 'tangzhengliang': 246, 'manuelab_telenav': 22, 'Markus59': 1, 'Little Brother': 27, 'AJMered1': 6, 'Dami_Tn': 10, 'mjn': 13, 'mueschel': 1, 'Carl Simonson': 1, 'zafkielknight': 24, 'RetiredInNH': 2, 'rolandg': 247, 'danhooker': 2, 'Dispersia': 75, 'FTA_dwg': 1, 'dmgctrl': 10, 'Chetan_Gowda': 17, 'matthewkenny': 4, 'TheDude05': 1, 'pyram': 255, 'Sean Robinson': 7, 'amillar': 68, 'Baroobob': 33, 'ccompanik': 677, 'dbdavies': 3, 'tlnd': 1, 'ninjamask': 1, 'RonRizzo': 2, 'Peter Fischer': 6, 'AndrewGG': 3, 'hmike12': 12, 'By-Tor': 13, 'Lazerhawk16': 162, 'dkokosky': 85, 'Peter Davies': 129, 'SimMoonXP': 3, 'MtarTDi': 89, 'ecloud': 57, 'RoadGeek_MD99': 19, 'Caboosey': 1, 'elmtreerecovery': 1, 'marinad_telenav': 3, 'Dero Bike Racks': 1, 'Milo': 46, 'iMarketSolutions': 1, 'jharpster': 112, 'NinjaDrew602': 57, 'rrbaker': 3, 'StellanL': 49, 'ScottsdaleBrian': 44, 'hobbesvsboyle': 13, 'srividya_c': 405, 'leosmoving': 1, 'scottsdaleapp': 1, 'Airlobster': 1, '2bre99': 699, 'alphasquid': 53, 'mvexel': 24, 'EmlynSquare': 8, 'DustoneGT': 161, 'RubyRide': 1, 'RalfZ': 1, 'aaronsta': 365, 'adenium': 6793, 'inah_telenav': 1911, 'MulletOnFire': 7, 'BharataHS': 1, 'EricAZ': 3, 'derricknehrenberg': 315, 'russdeffner': 15, 'Adam Martin': 7948, 'NayanataraM': 2, 'AKLAB': 1, 'Tushar Jain': 35, 'lorandr_telenav': 1, 'Gerard Jeronowitz': 36, 'Luke Stein': 12, 'Latze': 183, 'surveyor54': 1, 'scruss': 13, 'ebe': 2, '[atomic]': 4, 'Patoconnor43': 2, 'kookiemonster': 6728, 'mbiker': 1, 'pratikyadav': 19, 'GoWestTravel': 84, 'arizonastonecare': 1, 'malajul': 14, 'elbatrop': 1, 'chdr': 1, 'Chenke80': 13, 'AZ_Hiker': 3, 'MatthewAndersonUS80': 24, 'gausserrorfunction': 381, 'Professor_MC': 1, 'wallclimber21': 84, 'Jim Johansen': 9, 'Chanman': 2, 'KindredCoda': 194, 'squigglyspooge': 127, 'drewshep': 1, 'DaveHansenTiger': 33, u'Aurimas Fi\\u0161eras': 14, 'tobinlawoffice': 1, 'catswift26': 117, 'jebmpls': 2, 'colindt': 249, 'brogo': 1, 'Chumeg': 13, 'swissdreamer': 8, 'techlady': 573, 'jinalfoflia': 158, 'ParagonPrime': 525, 'PlaneMad': 120, 'maggot27': 1, 'gaku': 102, 'manoharuss': 11, 'data1': 42, 'Janjko': 21, 'rmessenger': 18, 'Rovastar': 1, 'RLugo': 1, 'blhack': 5, 'tomgenco': 79, 'Instinct Holiday': 40, 'James Fee': 278, 'ridixcr': 543, 'ylegkaya': 22, 'Nehaj': 63, 'Omnific': 21, 'MBonnice': 42, 'malcolmfrost': 15, 'Nlivoni': 45, 'MikeChuck': 158, 'djsnaxz': 1, 'psuprize': 1, 'Heinz_V': 3, 'adamos': 1, 'Tamires24': 22, 'lks1': 3, 'Omega-Pet': 2, 'dedNikifor': 5, 'hfyu': 9, 'der_kluge': 1, 'karitotp': 940, 'Matt Kaufman': 1, 'Chris Lawrence': 1441, 'ranjithjoy': 64, 'katiethomson': 6, 'autocorr': 83, 'Kittenz': 168, 'upendrakarukonda': 1, 'JaggedMind': 18, 'Bhojaraj': 22, 'whitemink': 122, 'hongshuyao': 92, 'sopen': 3, 'suncoastnails': 1, 'schugure': 1, 'claysmalley': 46, 'Clay Carroll': 5, 'cjfergus': 3, 'huggydog': 2, 'LXT': 1, 'remingtonhotels': 13, 'Mauls': 1, 'jkbroekhuizen': 10, 'Alan Bragg': 10, 'Mateusz Konieczny': 26, 'pflier': 4390, 'ereiamjh3': 2, 'ChrisCaspers': 3, 'Imp_GL': 25, 'dannykath': 1813, 'excelcollisioncenter': 1, 'Mikkel Kirkgaard Nielsen': 228, 'Bess Rebel': 1, 'Guddie': 4, 'Maarten Deen': 3, 'xylome': 1, 'maponpoint33': 3049, 'Jenny Crandall': 1, 'Leif Lodahl': 1322, 'brianegge': 4, 'ruph': 4, 'geozeisig': 1, 'Timothy Smith': 108, 'SomeoneElse_Revert': 4, 'Iqhra': 57, 'Map King': 54, 'drrv': 1, 'PeterNSteinmetz': 1, 'nusshkazn': 19, 'Ben Luo': 1, u'IbnT\\u0113\\u0161f\\u012bn': 4, 'msamples23': 1, 'scrottie': 3, 'Aroche': 4, 'dwh1985': 7404, 'DennisL': 29, 'ScottsdaleChurch': 1, 'Iowa Kid': 102, 'Cool_DPS': 122, 'Rory Nealon': 5, 'Dr Kludge': 120310, 'derFred': 1, 'Sunfishtommy': 497, 'wvdp': 11, 'balrog-kun': 137, 'tymillii': 11, 'venkanna37': 46, 'hno2': 36, 'americhemaz': 6, 'davmike': 1, 'gabis_telenav': 4, 'Paul Johnson': 1, 'tempebellanails': 1, 'David Maciaszek': 1, 'Luis36995': 1621, 'Outflow Marketing': 1, 'bahnpirat': 314, 'liamrocker': 1, 'Hvieira83': 2, 'calfarome': 1411, 'zheth': 30, 'ElliottPlack': 87, 'vipssy': 13, 'mapper999': 96, 'Aleks-Berlin': 31, 'ddcmarkw': 1, None: 714204, 'xBalthamel': 79, 'coffeeforclosers': 2, 'malenki': 18, 'hofoen': 37, 'CorranHorn': 2237, 'zaneslaw': 1, 'emjbe': 17, 'andreis_telenav': 41, 'har777': 24, 'The Temecula Mapper': 232, 'woodpeck_fixbot': 9543, 'Rudolf Mayer': 181, 'EdSS': 14, 'marthaleena': 6, 'Kurtjb': 1, 'sebastic': 170, 'BEHRBUT': 5, 'dima': 2, 'samely': 2382, '7im': 5, 'flierfy': 44, u'\\u0141ukasz Dudek': 1, 'effektz': 165, 'gormur': 32, 'petrar_telenav': 130, 'eric22': 1626, 'danidoedel': 2, 'Trex2001': 754, 'ToffeHoff': 94, 'snarfel': 2, 'Nhan Le': 2, 'Coffeemuggth': 2, 'Andre68': 1, 'samlarsen1': 6, 'Atom30': 63, 'ayushupreti22': 283, 'cdavila': 2, 'pete404': 11, 'SmartToaster': 10, 'Justas Birgiolas': 42, 'xe1gyp': 2, 'jakepruitt': 4, 'Ben97': 1, 'OSMJeff': 4, 'jumbanho': 319, 'jeespindola': 1, 'neufeind': 1, 'zephyr': 429, 'wangfengfight': 11, 'oanac2_telenav': 323, 'SteveC': 26, 'nvk': 5, 'compdude': 63, 'truebluepoolsarizona': 1, 'salix01': 1, 'dfellow': 735, 'RitterR': 14, 'sethlewis': 3, 'horndude77': 4419, 'jfuredy': 15633, 'vsnixon': 1, 'Hundehalter': 3, 'andrewpmk': 26, 'mJohnson89': 29, 'ianmcorvidae': 722, 'BekahElise': 1677, 'kalofxeno': 176, 'us-az-mesa-BVUG': 194, 'brandonpdx': 10, 'bdiscoe': 61, 'AJ Riley': 5018, 'poornibadrinath': 28, 'NorthHavenMapper': 1, 'R0bst3r': 1, 'Ronnoc324': 1, 'zz_top': 1, 'Claumires': 7, 'kisaa': 4, 'us-az-mesa-mcc': 668, 'ReB00t': 4, 'adjuva': 1, 'iandees': 115, 'us-az-mesa-BLUV': 1, 'dkunce': 4, 'camroidv27': 227, 'beej71': 11, 'CloCkWeRX': 135, 'wambacher': 7, 'HattoriHanzo': 1, 'baditaflorin': 662, 'houston_mapper1': 2, 'pablosanchez12': 34, 'cookry': 235, 'JamesKingdom': 217, 'JeffM8': 4, 'KR-KRKR-KR': 3, 'tfett': 24, 'Hayson337': 9, 'Globe4Change': 2, 'release_candidate': 1, 'dcp': 6, 'Chris Bell in California': 1065, 'nyuriks': 23, 'paulmach': 5, 'angrybirdseller': 540, 'boozyjenkins': 470, 'abel801': 137, 'CartoCrazy': 4, 'cmaz': 31, 'nammala': 164, 'asapserve': 1, 'ddffnn': 136, 'delayedmusic': 1, 'reunify_aarti': 4, 'user_5359': 858, 'bruchpilot': 1, 'Bike Mapper': 4810, 'PHerison': 5, 'arpconsulting': 15, 'cassini83': 30, 'calliaz': 1091, 'icaunais': 15, 'danielmoberly': 2734, 'Shaun@Asu': 1, 'JETnPHX': 3, 'Kurly': 3, 'scottstgelais': 1, 'Berjoh': 31, 'bhavana naga': 2, 'Thomas Hills': 5, 'Fa7C0N': 13, 'jgrnt': 1, 'Yoshinion': 47, 'team oggy': 166, 'MojaveNC': 104, 'moonwashed': 6, 'Squircle_': 10, 'GerdP': 121, 'William_Mancini': 67, 'osm-sputnik': 1, 'mash84121': 436, 'jhairehmyah': 839, 'newmedialab': 37, 'Temesy': 2, 'Austin Healey': 4, 'Ktr101': 1, 'riordan': 2, 'bungi72': 119, 'Bursera': 21, 'rad1ance': 29, 'piligab': 1052, 'TheArguer': 457, 'Christoph Lotz': 1, 'howdystranger': 40, 'marianp_telenav': 954, 'PawnNowScotts': 1, 'NE2': 422, 'NE3': 1, 'headwatersolver': 302, 'DrHog': 23, 'a6y': 12, 'Edward': 55, 'newantt': 1600, 'Chargerrt28': 22, 'Stephanie Landa': 8, 'TheDutchMan13': 44000, 'Rub21': 3416, 'corinag_telenav': 13, 'AMY-jin': 7, 'Peter14': 1, 'Umbugbene': 18, 'victoriajohnson': 5, 'Joe Schwartz': 6, 'Geogast': 44, 'KristenK': 18, 'theHulksToes': 229, 'delfman': 4, 'shravan91': 19, 'HurricaneCoast': 119, 'LCater88': 4, 'Peter Heck @ OnStar': 106, 'WorstFixer': 5, 'bluegroupfo': 1, 'qwerty10': 3, 'bogdanp_telenav': 308, 'Rbonifasi': 1, 'Sebastien Duthil': 1, 'snodnipper': 1, 'Coltron': 47, 'TeresaPeteti': 1}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "\n",
    "def get_user(element):\n",
    "    return element.get('user')\n",
    "\n",
    "count = 0\n",
    "\n",
    "def process_map(filename):\n",
    "    \n",
    "    user_count = {}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        \n",
    "        user_name = get_user(element)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if user_name in user_count:\n",
    "            user_count[user_name] += 1\n",
    "        else:\n",
    "            user_count[user_name] = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    return user_count\n",
    "\n",
    "user_count = process_map(file_path)\n",
    "\n",
    "\n",
    "print user_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c2e4bc29a406>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;31m# the problem here is the pattern is also picking up non street names such as #111\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m \u001b[0mst_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mst_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mways\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mst_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-c2e4bc29a406>\u001b[0m in \u001b[0;36maudit\u001b[1;34m(osmfile)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosmfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mosm_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosmfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mstreet_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosm_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "           'Ave': 'Avenue',\n",
    "           'Blvd.' : 'Boulevard',\n",
    "           'Pkwy' : 'Parkway',\n",
    "           'Rd.' : 'Road',\n",
    "           'Rd' : 'Road',\n",
    "           'Dr' : 'Drive',\n",
    "           'Rd,' : 'Road'\n",
    "          \n",
    "           \n",
    "            }\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    \n",
    "\n",
    "    if '#' in name:\n",
    "        name = name.split('#',-1)[0].strip()\n",
    "        \n",
    "    if ',' in name:\n",
    "        name = name.split(',', 1)[0].strip()\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    m = street_type_re.search(name)\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            if street_type in mapping:\n",
    "    \n",
    "                name= re.sub(street_type_re, mapping[street_type] , name)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    assert len(st_types) == 3\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name\n",
    "            if name == \"West Lexington St.\":\n",
    "                assert better_name == \"West Lexington Street\"\n",
    "            if name == \"Baldwin Rd.\":\n",
    "                assert better_name == \"Baldwin Road\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the problem here is the pattern is also picking up non street names such as #111\n",
    "st_types = audit(file_path)\n",
    "\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        print update_name(name, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing and shaping the data into CSV files\n",
    "**Note: the template of the code was provided by udacity quiz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 \n",
    "\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = 'Phoenix'\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    \n",
    "    way_attribs = {}\n",
    "    way_nodes_dict = {}\n",
    "    \n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    way_tag_dict = {}\n",
    "    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        for node in NODE_FIELDS:\n",
    "            node_attribs[node] = element.attrib[node]\n",
    "        for child in element:\n",
    "            tag_dict = {}\n",
    "            \n",
    "            if child.tag == 'tag':\n",
    "                tag_dict['id'] = element.attrib['id'] # set the id = parent id\n",
    "                \n",
    "                \n",
    "                \n",
    "                # and child.attrib['v'] here\n",
    "                tag_dict['value'] = child.attrib['v']\n",
    "                \n",
    "                n = re.search(PROBLEMCHARS, child.attrib['k'] )\n",
    "                m = re.search(LOWER_COLON, child.attrib['k'])\n",
    "                \n",
    "                if n:\n",
    "                    # you access child.attrib['k']\n",
    "                    tag_dict['key'] = child.attrib['k']\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "                elif m:\n",
    "                    # in case of colon in key, clean to standardize the data\n",
    "                    tag_dict['key'] = child.attrib['k'].split(':')[0]\n",
    "                    tag_dict['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                    \n",
    "                \n",
    "                else:\n",
    "                    # if it does not meet the above criteria, assign it as it is\n",
    "                    tag_dict['key'] = child.attrib['k']\n",
    "                    tag_dict['type'] = 'Regular'\n",
    "                \n",
    "                #print tag_dict works fine here\n",
    "                # append the result of dictionaries to tags list\n",
    "                # Note it is not appending as it supposed to\n",
    "                tags.append(tag_dict)\n",
    "        \n",
    "            \n",
    "\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        # implement way_attribs here\n",
    "        for way in WAY_FIELDS:\n",
    "            way_attribs[way] = element.attrib[way]\n",
    "            \n",
    "            \n",
    "            \n",
    "        # implement childern in elements here (way_nodes_dict)\n",
    "        counter = 0\n",
    "        for child in element:\n",
    "            if child.tag == 'tag':\n",
    "                tag = {'id': way_attribs['id']}\n",
    "                k = child.get('k')\n",
    "                if not PROBLEMCHARS.search(k):\n",
    "                    k = k.split(':', 1)\n",
    "                    tag['key'] = k[-1]\n",
    "                    tag['value'] = child.get('v')\n",
    "                    if len(k) == 1:\n",
    "                        tag['type'] = 'regular'    \n",
    "                    elif len(k) == 2:\n",
    "                        tag['type'] = k[0]\n",
    "                tags.append(tag)\n",
    "                \n",
    "                \n",
    "            if child.tag == 'nd':\n",
    "                nd = {'id' : way_attribs['id']}\n",
    "                nd['node_id'] = child.get('ref')\n",
    "                nd['position'] = counter\n",
    "                way_nodes.append(nd)\n",
    "            counter += 1\n",
    "                \n",
    "                \n",
    "        \n",
    "            \n",
    "        \n",
    "            \n",
    "                \n",
    "        \n",
    "                \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'wb') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'wb') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'wb') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'wb') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'wb') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "\n",
    "process_map(OSM_PATH, validate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting CSV files into SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function close>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3, csv\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "sqlite_file = 'mydb_NEW.db' # name of db file\n",
    "\n",
    "# connect to database:\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "conn.text_factory = str\n",
    "\n",
    "# Get a cursor object \n",
    "cur = conn.cursor() # Return a cursor for the connection.\n",
    "\n",
    "'''nodes_tags section'''\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags''') # exucutes an sql statement\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "# Create the table, specifying the column names and data types:\n",
    "cur.execute('''\n",
    "    CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT,type TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "# open nodes_tags.csv\n",
    "with open('nodes_tags.csv', 'rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# inseart the formatted data\n",
    "cur.executemany(\n",
    "    'INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);', to_db)\n",
    "    \n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' nodes section'''\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes''') # exucutes an sql statement\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE nodes(id INTEGER, lat DECIMAL, lon DECIMAL, user TEXT, uid INTEGER, version INTEGER , changeset INTEGER, timestamp TIMESTAMP)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "# Read the csv file as a dictionary, \n",
    "# then format as a list of tuples\n",
    "\n",
    "# open nodes.csv\n",
    "# id\tlat\tlon\tuser\tuid\tversion\tchangeset\ttimestamp\n",
    "with open('nodes.csv', 'rb') as fin:\n",
    "    dr = csv.DictReader(fin)\n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "cur.executemany('INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?,?,?);', to_db)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' ways section'''\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways''') # exucutes an sql statement\n",
    "conn.commit()\n",
    "\n",
    "#id\tuser\tuid\tversion\tchangeset\ttimestamp\n",
    "\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways(id INTEGER, user TEXT, uid INTEGER, version INTEGER , changeset INTEGER, timestamp TIMESTAMP)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "# Read the csv file as a dictionary, \n",
    "# then format as a list of tuples\n",
    "\n",
    "# open ways.csv\n",
    "with open('ways.csv', 'rb') as fin:\n",
    "    dr = csv.DictReader(fin)\n",
    "    to_db = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "cur.executemany('INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?);', to_db)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' ways nodes section'''\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes''') # exucutes an sql statement\n",
    "conn.commit()\n",
    "\n",
    "#id\tnode_id\tposition\n",
    "\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways_nodes(id INTEGER, node_id INTEGER, position INTEGER)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "# Read the csv file as a dictionary, \n",
    "# then format as a list of tuples\n",
    "\n",
    "# open ways_nodes.csv\n",
    "with open('ways_nodes.csv', 'rb') as fin:\n",
    "    dr = csv.DictReader(fin)\n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "    \n",
    "cur.executemany('INSERT INTO ways_nodes(id, node_id, position) VALUES (?,?,?);', to_db)\n",
    "\n",
    "\n",
    "\n",
    "''' ways tags section'''\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_tags''') # exucutes an sql statement\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "#id\tkey\tvalue\ttype\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways_tags(id INTEGER, key TEXT, value TEXT, type TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "# Read the csv file as a dictionary, \n",
    "# then format as a list of tuples\n",
    "\n",
    "\n",
    "with open('ways_tags.csv', 'rb') as fin:\n",
    "    dr = csv.DictReader(fin)\n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "cur.executemany('INSERT INTO ways_tags(id, key, value, type) VALUES (?,?,?,?);', to_db)\n",
    "\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
